{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Straylight\n",
    "\n",
    "### Common Crawl Domain Reconnaissance\n",
    "\n",
    "__Introduction:__\n",
    "This notebook will provide the ability to configure and search the publicly available Common Crawl dataset of websites. Common Crawl is a freely available dataset which contains over 8 years of crawled data including over 25 billion websites, trillions of links, and petabytes of data.\n",
    "\n",
    "__GitHub:__\n",
    "* https://github.com/brevityinmotion/straylight\n",
    "\n",
    "__Blog:__\n",
    "* [Search the html across 25 billion websites for passive reconnaissance using common crawl](https://medium.com/@brevityinmotion/search-the-html-across-25-billion-websites-for-passive-reconnaissance-using-common-crawl-7fe109250b83?sk=5b8b4a7c506d5acba572c0b30137f7aa)\n",
    "\n",
    "___Credits:___\n",
    "* Special thank you to Sebastian Nagel for the tutorials and insight for utilizing the dataset!\n",
    "* Many of the functions and code have been adapted from: http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior to utilizing this notebook, the following three queries should be run within AWS Athena to configure the Common Crawl database.\n",
    "\n",
    "#### Query 1\n",
    "<code>CREATE DATABASE ccindex</code>\n",
    "\n",
    "#### Query 2\n",
    "<code>CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (\n",
    "  url_surtkey                   STRING,\n",
    "  url                           STRING,\n",
    "  url_host_name                 STRING,\n",
    "  url_host_tld                  STRING,\n",
    "  url_host_2nd_last_part        STRING,\n",
    "  url_host_3rd_last_part        STRING,\n",
    "  url_host_4th_last_part        STRING,\n",
    "  url_host_5th_last_part        STRING,\n",
    "  url_host_registry_suffix      STRING,\n",
    "  url_host_registered_domain    STRING,\n",
    "  url_host_private_suffix       STRING,\n",
    "  url_host_private_domain       STRING,\n",
    "  url_protocol                  STRING,\n",
    "  url_port                      INT,\n",
    "  url_path                      STRING,\n",
    "  url_query                     STRING,\n",
    "  fetch_time                    TIMESTAMP,\n",
    "  fetch_status                  SMALLINT,\n",
    "  content_digest                STRING,\n",
    "  content_mime_type             STRING,\n",
    "  content_mime_detected         STRING,\n",
    "  content_charset               STRING,\n",
    "  content_languages             STRING,\n",
    "  warc_filename                 STRING,\n",
    "  warc_record_offset            INT,\n",
    "  warc_record_length            INT,\n",
    "  warc_segment                  STRING)\n",
    "PARTITIONED BY (\n",
    "  crawl                         STRING,\n",
    "  subset                        STRING)\n",
    "STORED AS parquet\n",
    "LOCATION 's3://commoncrawl/cc-index/table/cc-main/warc/';</code>\n",
    "\n",
    "#### Query 3\n",
    "<code>MSCK REPAIR TABLE ccindex</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accelerator provides a function to retrieve a secret value from AWS secrets manager. The secret name along with the region must be passed into the function as parameters. The code has been adapted into a parameterized function from the canned template provided by AWS.\n",
    "import json, boto3, time, requests, io, base64\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_secret(secret_name, region_name):\n",
    "\n",
    "    #secret_name = \"AmazonSageMaker-gmaps\"\n",
    "    #region_name = \"us-east-2\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            #secret = json.loads(secret)\n",
    "            return json.loads(secret)\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return json.loads(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accelerator provides a standardized function for passing queries to Athena.\n",
    "\n",
    "def queryathena(athenadb, athenabucket, query):\n",
    "    athena = boto3.client('athena', region_name='us-east-1')\n",
    "    qexec = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database':athenadb\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation':athenabucket\n",
    "        }\n",
    "    )\n",
    "    execid = qexec['QueryExecutionId']\n",
    "    return execid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accelerator provides a standardized pattern for retrieving Athena query results based on the execution id.\n",
    "# This code is adapted from Evan Perotti from http://securityriskadvisors.com/blog/creating-a-project-sonar-fdns-api-with-aws/ and was adapted from the Lambda.\n",
    "\n",
    "def retrieveresults(execid):\n",
    "    athena = boto3.client('athena', region_name='us-east-1')\n",
    "    s3 = boto3.client('s3')\n",
    "    queryres = athena.get_query_execution(\n",
    "        QueryExecutionId = execid\n",
    "    )\n",
    "    \n",
    "    # Athena query checking code is from https://medium.com/dataseries/automating-athena-queries-from-s3-with-python-and-save-it-as-csv-8917258b1045\n",
    "    # Loop until results are ready or fail after 5 minutes\n",
    "    status = 'RUNNING'\n",
    "    iterations = 60\n",
    "    \n",
    "    while (iterations>0):\n",
    "        iterations = iterations - 1\n",
    "        response_get_query_details = athena.get_query_execution(\n",
    "        QueryExecutionId = execid\n",
    "        )\n",
    "        status = response_get_query_details['QueryExecution']['Status']['State']\n",
    "        print(status)\n",
    "        if (status == 'FAILED') or (status == 'CANCELLED'):\n",
    "            return False, False\n",
    "        elif status == 'SUCCEEDED':\n",
    "            try:\n",
    "                outputloc = queryres['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "                full = outputloc[5:] # trim s3:// prefix\n",
    "                bucketloc = full.split('/')[0] # get bucket from full path\n",
    "                keyloc = full.replace(bucketloc,'')[1:] # get key and remove starting /\n",
    "    \n",
    "                url = s3.generate_presigned_url(\n",
    "                    'get_object',\n",
    "                    Params={\n",
    "                    'Bucket':bucketloc,\n",
    "                    'Key':keyloc\n",
    "                    }\n",
    "                )\n",
    "                return url\n",
    "            except:\n",
    "                url = \"No results\"\n",
    "                return url\n",
    "        else:\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to update these values\n",
    "DOMAIN_TO_QUERY = 'thefarmersjournal.com' # This should look like 'domain.com'. The wildcard will be added automatically later.\n",
    "ATHENA_BUCKET = 's3://cc.index.gkn0672/' # This will need to be customized and specific to your own account (i.e. s3://customname-athena').\n",
    "ATHENA_DB = 'ccindex' # This should align with the database and not need changed if it was created using the previous queries.\n",
    "ATHENA_TABLE = 'ccindex' # This should align with the table and not need changed if it was created using the previous queries.\n",
    "\n",
    "# Do not modify this query unless the intent is to customize\n",
    "query = \"SELECT url, url_query, warc_filename, warc_record_offset, warc_record_length, fetch_time FROM %s WHERE subset = 'warc' AND url_host_registered_domain = '%s';\" % (ATHENA_TABLE, DOMAIN_TO_QUERY)\n",
    "\n",
    "execid = queryathena(ATHENA_DB, ATHENA_BUCKET, query)\n",
    "print(execid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, time, requests\n",
    "import pandas as pd\n",
    "import io\n",
    "    \n",
    "# Utilize executionID to retrieve results\n",
    "downloadURL = retrieveresults(execid)\n",
    "\n",
    "# Load output into dataframe\n",
    "s=requests.get(downloadURL).content\n",
    "dfhosts=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates keeping the latest version - if you want to review changes between fetch, you may not want to run this\n",
    "dfhosts = dfhosts.sort_values('fetch_time').drop_duplicates('url',keep='last',ignore_index=True)\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to excel spreadsheet\n",
    "dfhosts['url'].to_csv(\"cc-urls-loigiaihay.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "dfhosts['url'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warcio\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4 import Comment\n",
    "import io\n",
    "from io import BytesIO\n",
    "import logging\n",
    "import dask.dataframe as ddf\n",
    "import multiprocessing\n",
    "import dask.bag as db\n",
    "import gc\n",
    "\n",
    "# Thank you to Sebastian Nagel for your instructions and code to perform the following step.\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "titles_list = []\n",
    "uris_list = []\n",
    "links_list = []\n",
    "comments_list = []\n",
    "body_list = []\n",
    "\n",
    "#Fetch all WARC records defined by filenames and offsets in rows, parse the records and the contained HTML, split the text into words and emit pairs <word, 1>\n",
    "def processwarcrecords(dfhosts, writefiles, howmanyrecords):\n",
    "    s3client = boto3.client('s3')\n",
    "    recordcount = 0\n",
    "    skippedrecords = 0\n",
    "    processedrecords = 0\n",
    "    totalrecords = len(dfhosts.index)\n",
    "    if howmanyrecords == 0:\n",
    "        howmanyrecords = totalrecords\n",
    "    for index, row in dfhosts.iterrows():\n",
    "        if recordcount > howmanyrecords:\n",
    "            break\n",
    "        recordcount = recordcount + 1\n",
    "        # print('Processing row ' + str(recordcount) + ' of ' + str(totalrecords) + ' total rows.')\n",
    "        if recordcount % 1000 == 0:\n",
    "            print('Processed ' + str(processedrecords) + ' records.')\n",
    "        url = row['url']\n",
    "        warc_path = row['warc_filename']\n",
    "        offset = int(row['warc_record_offset'])\n",
    "        length = int(row['warc_record_length'])\n",
    "        rangereq = 'bytes={}-{}'.format(offset, (offset+length-1))\n",
    "        filepath = os.path.join(os.getcwd(), 'tmp', url.replace(\"https://\",\"\").replace(\"http://\",\"\") + str(offset) + '.html')\n",
    "         # Check if the file already exists\n",
    "        if os.path.exists(filepath):\n",
    "            # print('File exists: ' + filepath)\n",
    "            skippedrecords = skippedrecords + 1\n",
    "            if skippedrecords % 1000 == 0:\n",
    "                print('Skipped ' + str(skippedrecords) + ' records.')\n",
    "            continue  # Skip processing this record\n",
    "        response = s3client.get_object(Bucket='commoncrawl',Key=warc_path,Range=rangereq)\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 60  # seconds\n",
    "\n",
    "        for retry_count in range(max_retries):\n",
    "            try:\n",
    "                response = s3client.get_object(Bucket='commoncrawl', Key=warc_path, Range=rangereq)\n",
    "                # Process the response\n",
    "                break  # Break out of the retry loop if successful\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                time.sleep(retry_delay * 2**retry_count)\n",
    "\n",
    "        record_stream = BytesIO(response[\"Body\"].read())\n",
    "        for record in ArchiveIterator(record_stream):\n",
    "            if record.rec_type == 'response':\n",
    "                try:\n",
    "                    warc_target_uri = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                    page = record.content_stream().read()\n",
    "                    soup = BeautifulSoup(page, 'html.parser') # lxml should be faster but is not\n",
    "                    title = soup.title.string\n",
    "                    titles_list.append((warc_target_uri, title))\n",
    "                    uris_list.append((warc_target_uri))\n",
    "\n",
    "                    # Find all links\n",
    "                    for link in soup.find_all('a'):\n",
    "                        links_list.append((warc_target_uri, link.get('href')))\n",
    "                    # Find all links\n",
    "                    for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "                        comments_list.append((warc_target_uri, comment))\n",
    "                    # Find all body text\n",
    "                    for body in soup.find_all('body'):\n",
    "                        body_list.append((warc_target_uri, body))\n",
    "\n",
    "                    if writefiles == 'yes':\n",
    "                        page = page.decode(\"utf-8\") \n",
    "                        url = url.replace(\"https://\",\"\")\n",
    "                        url = url.replace(\"http://\",\"\")\n",
    "                        url = url + str(offset) + '.html'\n",
    "                        filepath = os.getcwd() + '/tmp/' + url\n",
    "                        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "                        if os.path.exists(filepath):\n",
    "                        # print('File exists: ' + filepath)\n",
    "                            skippedrecords = skippedrecords + 1\n",
    "                            if skippedrecords % 1000 == 0:\n",
    "                                print('Skipped ' + str(skippedrecords) + ' records.')\n",
    "\n",
    "                        else:\n",
    "                            with open(filepath, \"w\", encoding='utf-8') as text_file:\n",
    "                                text_file.write(soup.prettify())\n",
    "                                processedrecords = processedrecords + 1\n",
    "                    # Clear memory after processing each record\n",
    "                    del page, soup, title\n",
    "                    gc.collect()\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger = logging.getLogger('errorhandler')\n",
    "                    print(logger.error('Error: '+ str(e)))\n",
    "                    skippedrecords = skippedrecords + 1\n",
    "                    print('Skipped ' + str(skippedrecords) + ' records.')\n",
    "            del response, record_stream\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "# searchfiles = 'yes' # anything other than 'yes' will not process\n",
    "writefiles = 'yes' # anything other than 'yes' will not process\n",
    "howmanyrecords = 0 # 0 is all records; other options would be a numeric value\n",
    "processwarcrecords(dfhosts,writefiles,howmanyrecords)\n",
    "\n",
    "df_dask = ddf.from_pandas(dfhosts, npartitions=6)   # where the number of partitions is the number of cores you want to use\n",
    "df_dask.apply(lambda x: processwarcrecords(dfhosts,writefiles,howmanyrecords), axis=1, meta=('str')).compute(scheduler='multiprocessing')\n",
    "# df_dask['output'] = df_dask.apply(lambda x: (x), meta=('str')).compute(scheduler='multiprocessing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lists into dataframes for further processing\n",
    "dfcomments = pd.DataFrame(comments_list,columns=['URI','Comment'])\n",
    "dftitles = pd.DataFrame(titles_list,columns=['URI','Title'])\n",
    "dflinks = pd.DataFrame(links_list,columns=['URI','Link'])\n",
    "dfbody = pd.DataFrame(body_list,columns=['URI','Body'])\n",
    "#dflinks.head(10)\n",
    "dfcomments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for keywords within the comments\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "search_values = []\n",
    "dfcomments[dfcomments.Comment.str.contains('|'.join(search_values ),flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export html search results to excel\n",
    "\n",
    "#with pd.ExcelWriter('cc-domains.xlsx') as writer:  \n",
    "#    dfcomments.to_excel(writer, sheet_name='comments')\n",
    "#    dftitles.to_excel(writer, sheet_name='titles')\n",
    "#    dflinks.to_excel(writer, sheet_name='links')\n",
    "\n",
    "# if dataframe has over 65535 rows, Excel will skip data. In this situation, .csv is better.\n",
    "#compression_opts = dict(method='zip',archive_name='out.csv')  \n",
    "dfcomments.to_csv('dfcomments.csv', header=True, index=False)\n",
    "dftitles.to_csv('dftitles.csv', header=True, index=False) \n",
    "dflinks.to_csv('dflinks.csv', header=True, index=False) \n",
    "dfbody.to_csv('dfbody.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zip the file for download out of Jupyter\n",
    "filepath = os.getcwd() + '/tmp/'\n",
    "# This will create a file named domainoutput.tar.gz with the full html files in the structure of the website. It can be downloaded from the same directory running the notebook.\n",
    "! tar -zcvf domainoutput.tar.gz $filepath\n",
    "# This will clean-up the tmp folder\n",
    "! rm -rf tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional example queries to run with this configuration.\n",
    "# These can be run directly within the Athena query window in the AWS console or can be integrated into this notebook instead of using the pre-defined query.\n",
    "\n",
    "# Search the entire common crawl data set for specific URL parameters.\n",
    "SELECT url,\n",
    "       warc_filename,\n",
    "       warc_record_offset,\n",
    "       warc_record_length,\n",
    "       url_query\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE subset = 'warc'\n",
    "  AND url_query like 'cmd='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct websites for a specific domain\n",
    "\n",
    "SELECT DISTINCT COUNT(url) as URLCount\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE  subset = 'warc'\n",
    "  AND url_host_registered_domain = 'domain.com'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
